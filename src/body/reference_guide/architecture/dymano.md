# Dynamo

Apache Cassandra запозичує низку технік з розподіленої системи зберігання ключ-значення [Amazon Dynamo](https://www.cs.cornell.edu/courses/cs5414/2017fa/papers/dynamo.pdf). Кожен вузол у системі Dynamo має три основні компоненти:

- Координація запитів через розподілену вибірку даних
- Членство у кільці та виявлення відмов
- Локальний механізм збереження даних

Cassandra в основному використовує перші два компоненти кластеризації, а також механізм збереження даних, заснований на дереві злиття з журналом (Log Structured Merge Tree, LSM). Зокрема, Cassandra спирається на такі стилі Dynamo:

- Розподіл набору даних із використанням консистентного хешування
- Реплікація за принципом "мульти-мастер" з використанням версіонованих даних і налаштовуваної консистентності
- Розподілене членство в кластері та виявлення відмов за допомогою протоколу "госіпінгу" (gossip)
- Інкрементальне масштабування на звичайному апаратному забезпеченні

Cassandra була спроектована таким чином для задоволення вимог до зберігання даних великого масштабу (петабайти і більше), які є критичними для бізнесу. Зокрема, оскільки додатки вимагали повної глобальної реплікації даних на пета- та ексабайтових рівнях разом з завжди доступними, низьколатентними записами та зчитуваннями, стало необхідним розробити нову модель бази даних, оскільки реляційні бази даних того часу не могли задовольнити нові вимоги додатків глобального масштабу.

## Розподіл набору даних: Консистентне хешування

Cassandra досягає горизонтального масштабування, розподіляючи всі дані, що зберігаються в системі, за допомогою хеш-функції. Кожен розподіл реплікується на кількох фізичних вузлах, часто через домени відмов, такі як стійки або навіть дата-центри. Оскільки кожен репліка може незалежно приймати зміни для кожного ключа, який він володіє, кожен ключ повинен бути версіонованим. На відміну від оригінальної моделі Dynamo, де використовувалися детерміновані версії і векторні годинники для узгодження одночасних оновлень ключа, Cassandra використовує простішу модель "останній запис виграє", де кожна зміна має часову мітку (включаючи видалення), а остання версія даних є "виграшним" значенням. Формально, Cassandra використовує тип даних CRDT (Conflict-Free Replicated Data Type) для кожного рядка CQL — LWW-Element-Set (Last-Write-Wins Element-Set), щоб вирішувати конфлікти змін на наборах реплік.

### Консистентне хешування з використанням кільця токенів

Cassandra розподіляє дані між вузлами зберігання, використовуючи спеціальну форму хешування, звану консистентним хешуванням. У наївному хешуванні даних зазвичай розподіляють ключі по кошикам, використовуючи хеш ключа за модулем кількості кошиків. Наприклад, якщо ви хочете розподілити дані на 100 вузлів за допомогою наївного хешування, ви можете призначити кожен вузол до кошика від 0 до 100, хешувати вхідний ключ за модулем 100 і зберігати дані в асоційованому кошику. Проте в цій наївній схемі додавання одного вузла може порушити майже всі відображення.

Cassandra замість цього відображає кожен вузол на один або кілька токенів на безперервному хеш-кільці та визначає приналежність, хешуючи ключ на кільце і потім "рухаючи" кільце в одному напрямку, подібно до алгоритму [Chord](https://pdos.csail.mit.edu/papers/chord:sigcomm01/chord_sigcomm.pdf). Основна різниця консистентного хешування від наївного хешування даних полягає в тому, що коли кількість вузлів (кошиків) змінюється, консистентне хешування змушене переміщати лише невелику частину ключів.

Наприклад, якщо ми маємо кластер з восьми вузлів з рівномірно розташованими токенами і фактор реплікації (RF) рівний 3, то для знаходження вузлів, що володіють ключем, ми спочатку хешуємо цей ключ для отримання токена (що є просто хешем ключа), а потім "рухаємо" кільце за годинниковою стрілкою, поки не знайдемо три різні вузли, на яких зберігаються всі репліки цього ключа. Цей приклад можна візуалізувати таким чином:

![Ring](ring.svg)

В Dynamo-подібній системі ділянки ключів, відомі також як діапазони токенів, відповідають тому самому фізичному набору вузлів. У цьому прикладі всі ключі, які потрапляють до діапазону токенів, виключаючи токен 1 і включаючи токен 2 (grange(t1, t2]), зберігаються на вузлах 2, 3 і 4.

Це пояснення розподілу даних та реплікації в Cassandra через консистентне хешування та застосування токенів допомагає зрозуміти, як Cassandra забезпечує масштабованість і надійність своїх операцій.

### Кілька токенів на фізичному вузлі (vnodes)

Просте хешування з одним токеном добре працює, якщо у вас багато фізичних вузлів для розподілу даних, але при рівномірно розподілених токенах і малій кількості фізичних вузлів додавання нових вузлів (масштабування) стає складним, оскільки для нових вузлів немає доступних місць для токенів, які могли б підтримувати баланс кільця. Cassandra намагається уникати дисбалансу токенів, оскільки нерівномірні діапазони токенів призводять до нерівномірного навантаження запитів. Наприклад, у попередньому прикладі немає можливості додати дев'ятий токен без порушення балансу; замість цього нам доведеться вставити 8 токенів у середині існуючих діапазонів.

У статті Dynamo пропонується використовувати "віртуальні вузли" для вирішення проблеми дисбалансу. Віртуальні вузли вирішують цю проблему, призначаючи кілька токенів на кільце токенів кожному фізичному вузлу. Дозволяючи одному фізичному вузлу займати кілька позицій у кільці, ми можемо зробити маленькі кластери виглядати більшими і, отже, навіть при додаванні одного фізичного вузла, він виглядатиме так, ніби ми додали набагато більше вузлів, фактично забираючи менші шматки даних від більше кількості сусідів на кільці, коли додається навіть один вузол.

Cassandra вводить терміни для обробки цих концепцій:

- Токен: Одна позиція на хеш-кільці стилю Dynamo.
- Точка доступу (Endpoint): Одна фізична IP-адреса та порт у мережі.
- Ідентифікатор вузла (Host ID): Унікальний ідентифікатор для одного "фізичного" вузла, зазвичай присутній в одній точці доступу та містить один або більше токенів.
- Віртуальний вузол (або vnode): Токен на хеш-кільці, що належить тому самому фізичному вузлу, з тим самим ідентифікатором вузла.

Мапування токенів на точки доступу створює Токен Мапу, де Cassandra відстежує, які позиції кільця відповідають яким фізичним точкам доступу. Наприклад, у наведеному нижче прикладі ми можемо представити кластер з восьми вузлів, використовуючи лише чотири фізичні вузли, призначивши кожному вузлу два токени:

![vnodes](vnodes.svg)

Переваги кількох токенів на фізичному вузлі:

- Коли додається новий вузол, він приймає приблизно рівні обсяги даних від інших вузлів у кільці, що забезпечує рівномірний розподіл даних по всьому кластеру.
- Коли вузол виводиться з експлуатації, він втрачає дані рівномірно між іншими членами кільця, що знову забезпечує рівномірний розподіл даних.
- Якщо вузол стає недоступним, навантаження запитів (особливо на запити, які враховують токени) рівномірно розподіляється між багатьма іншими вузлами.

Недоліки кількох токенів:

- Кожен токен вводить до 2 * (RF - 1) додаткових сусідів на кільці токенів, що означає, що є більше комбінацій відмов вузлів, через які ми втрачаємо доступність частини кільця токенів. Чим більше токенів, тим вища ймовірність відключення.
- Операції з обслуговування кластера можуть сповільнюватися. Наприклад, чим більше токенів на вузол, тим більше дискретних операцій з ремонту має виконати кластер.
- Продуктивність операцій, що охоплюють діапазони токенів, може знизитися.

Зверніть увагу, що в Cassandra 2.x єдиним алгоритмом розподілу токенів було випадкове призначення токенів, що означало, що для збереження балансу необхідно було використовувати досить велику кількість токенів на вузол — 256. Це призводило до того, що багато фізичних точок доступу "з’єднувалися", збільшуючи ризик недоступності. Саме тому в 3.x + був доданий новий детерміністичний аллокатор токенів, який інтелектуально вибирає токени таким чином, щоб кільце було оптимально збалансоване, при цьому вимагалося значно менше токенів на фізичний вузол.

## Реплікація за принципом "мульти-мастер"

Cassandra реплікує кожну частину даних на кілька вузлів кластера для забезпечення високої доступності та стійкості даних. Коли відбувається зміна (мутація), координатор виконує хешування ключа розділу, щоб визначити діапазон токенів, до якого належать ці дані, і потім реплікує зміну на репліки цих даних відповідно до `стратегії реплікації`.

Усі стратегії реплікації передбачають поняття фактора реплікації (RF), який вказує Cassandra, скільки копій розділу має існувати. Наприклад, при RF=3 у просторі ключів, дані будуть записані на три різні репліки. Репліки завжди обираються так, щоб вони знаходились на різних фізичних вузлах, що досягається шляхом пропускання віртуальних вузлів, якщо це необхідно. Стратегії реплікації також можуть обирати пропускати вузли, що знаходяться в одному домені відмов, як-то стійки або датацентри, щоб кластери Cassandra могли витримувати відмови цілих стійок і навіть датацентрів вузлів.

### Стратегії Реплікації

Cassandra підтримує змінні стратегії реплікації, які визначають, які фізичні вузли діятимуть як репліки для даного діапазону токенів. Кожен простір ключів має свою стратегію реплікації. У всіх виробничих установках слід використовувати `NetworkTopologyStrategy`, тоді як `SimpleStrategy` корисна лише для тестування кластерів, де ще не відомо розташування датацентрів.

#### `NetworkTopologyStrategy`
`NetworkTopologyStrategy` вимагає визначення фактора реплікації для кожного датацентру в кластері. Навіть якщо ваш кластер використовує лише один датацентр, рекомендується використовувати `NetworkTopologyStrategy` замість `SimpleStrategy`, оскільки це полегшує додавання нових фізичних або віртуальних датацентрів до кластера, якщо це буде потрібно.

Окрім того, що дозволяється вказати фактор реплікації для кожного датацентру окремо, `NetworkTopologyStrategy` також намагається вибирати репліки всередині датацентру з різних стійок, як це вказано за допомогою `Snitch`. Якщо кількість стійок більша або дорівнює фактору реплікації для датацентру, кожна репліка буде гарантовано вибрана з різної стійки. В іншому випадку кожна стійка міститиме хоча б одну репліку, але деякі стійки можуть містити більше ніж одну репліку. Зазначте, що така поведінка щодо стійок може мати деякі [несподівані наслідки](https://cassandra.apache.org/doc/latest/cassandra/architecture/dynamo.html). Наприклад, якщо кількість вузлів у кожній стійці не є парною, навантаження на найменшу стійку може бути значно вищим. Також, якщо в нову стійку додається лише один вузол, цей вузол буде вважатися реплікою для всього кільця. З цієї причини багато операторів налаштовують усі вузли в межах однієї зони доступності або подібного домену відмов як одну "стійку".

#### `SimpleStrategy`
`SimpleStrategy` дозволяє визначити єдиний цілий `фактор реплікації`, який вказує, скільки вузлів має містити копію кожного рядка. Наприклад, якщо `фактор реплікації` дорівнює 3, то три різні вузли зберігатимуть копію кожного рядка.

`SimpleStrategy` поводиться з усіма вузлами однаково, ігноруючи будь-які налаштовані датацентри чи стійки. Щоб визначити репліки для діапазону токенів, Cassandra проходить через токени на кільці, починаючи з діапазону токенів, що цікавить. Для кожного токену перевіряється, чи був вузол, що володіє цим токеном, доданий до набору реплік, і якщо він ще не доданий, то додається. Цей процес триває, поки не буде додано необхідну `кількість реплік`.

#### *Транзитна Реплікація*

Транзитна реплікація — це експериментальна функція в Cassandra 4.0, якої не було в оригінальній статті Dynamo. Ця функція дозволяє налаштовувати підмножину реплік, які реплікують тільки ті дані, що ще не були поступово відновлені (repaired). Це дозволяє відокремити надмірність даних від доступності. Наприклад, якщо ви маєте простір ключів з фактором реплікації RF=3 і змінюєте його на RF=5 з двома транзитними репліками, ви збільшуєте можливість витримати дві відмови реплік замість однієї, не збільшуючи при цьому обсяг збереження даних. Тепер три вузли реплікують усі дані для певного діапазону токенів, а інші два реплікують лише дані, що ще не були поступово відновлені.

Для використання транзитної реплікації, спершу потрібно увімкнути цю опцію в конфігураційному файлі `cassandra.yaml`. Після цього можна налаштувати як `SimpleStrategy`, так і `NetworkTopologyStrategy` для транзитної реплікації даних, вказавши фактор реплікації у вигляді `<total_replicas>/<transient_replicas>`.

Транзитно репліковані простори ключів підтримують лише таблиці, створені з параметром `read_repair` встановленим на `NONE`; монотонні читання наразі не підтримуються. Також не можна використовувати `LWT`, зафіксовані пакети або лічильники в Cassandra 4.0. ймовірно, що матеріалізовані уявлення та вторинні індекси також не будуть підтримуватися для транзитно реплікованих просторів ключів.

Транзитна реплікація є експериментальною функцією, яка наразі не готова до використання в продуктивному середовищі. Очікувані додаткові можливості у версії 4.next включають підтримку монотонних читань з транзитною реплікацією, а також LWT, зафіксованих пакетів і лічильників.

### Версіонування Даних

Cassandra використовує версіонування з допомогою часових міток мутацій для забезпечення остаточної консистентності даних. Усі мутації, які надходять в систему, отримують часову мітку, що надається або годинником клієнта, або, якщо клієнтська мітка відсутня, годинником координатора. Оновлення вирішуються відповідно до правила вирішення конфліктів "останнє записує перше" (last write wins). Правильність роботи Cassandra залежить від цих годинників, тому важливо, щоб у системі була налаштована належна синхронізація часу, наприклад, за допомогою NTP.

Cassandra застосовує окремі часові мітки для кожної мутації кожного стовпця в кожному рядку всередині CQL розділу. Рядки гарантовано будуть унікальними за допомогою первинного ключа, і кожен стовпець у рядку вирішує одночасні мутації згідно з правилом "останнє записує перше". Це означає, що оновлення для різних первинних ключів у межах одного розділу можуть бути вирішені без конфліктів! Крім того, колекційні типи CQL, такі як карти та множини, використовують цей самий механізм безконфліктного вирішення, що гарантує, що одночасні оновлення карт і множин також будуть вирішені.

#### Синхронізація Реплік

Оскільки репліки в Cassandra можуть приймати мутації незалежно одна від одної, деякі репліки можуть мати новіші дані, ніж інші. Cassandra використовує кілька технік для сприяння досягненню конвергенції реплік, зокрема `зчитування реплік з виправленням` (`read-repair`) в процесі читання та `передача підказок` (`hinted handoff`) в процесі запису.

Ці техніки є лише намаганням досягти результату, однак, для забезпечення остаточної консистентності, Cassandra реалізує механізм `анти-ентропії` (`repair`), в якому репліки обчислюють ієрархічні хеш-дерева своїх даних, відомі як дерева Меркле (Merkle trees), які потім порівнюються між репліками для виявлення невідповідних даних. Як і в оригінальній статті Dynamo, Cassandra підтримує повні ремонти, коли репліки хешують весь свій набір даних, створюють дерева Меркле, передають їх між собою та синхронізують діапазони, які не збігаються.

На відміну від оригінальної статті Dynamo, Cassandra також реалізує ремонт піддіапазонів і інкрементний ремонт. Ремонт піддіапазонів дозволяє Cassandra збільшити точність хеш-дерев (можливо до рівня окремих розділів), створюючи більшу кількість дерев, що охоплюють лише частину діапазону даних. Інкрементний ремонт дозволяє Cassandra ремонтувати тільки ті розділи, які змінилися з моменту останнього ремонту.

### Налаштовувана Консистентність

Cassandra підтримує можливість налаштування компромісу між консистентністю та доступністю для кожної операції через рівні консистентності (Consistency Levels). Рівні консистентності в Cassandra — це версія механізму `R + W > N` з Dynamo, де оператори можуть налаштувати кількість вузлів, які повинні брати участь у операціях читання (`R`) і запису (`W`), що перевищує коефіцієнт реплікації (`N`). У Cassandra оператор обирає з набору стандартних рівнів консистентності, які дозволяють вибирати поведінку `R` та `W` без знання коефіцієнта реплікації. Зазвичай записи стають видимими для подальших читань, коли рівень консистентності читання включає достатньо вузлів для гарантії перетину з рівнем консистентності запису.

Доступні рівні консистентності:

- `ONE`: Повинен відповісти лише один репліка.
- `TWO`: Повинні відповісти дві репліки.
- `THREE`: Повинні відповісти три репліки.
- `QUORUM`: Більшість (n/2 + 1) реплік повинні відповісти.
- `ALL`: Повинні відповісти всі репліки.
- `LOCAL_QUORUM`: Більшість реплік у локальному датацентрі (у якому знаходиться координатор) повинні відповісти.
- `EACH_QUORUM`: Більшість реплік у кожному датацентрі повинні відповісти.
- `LOCAL_ONE`: Лише одна репліка повинна відповісти. У багатодатацентровому кластері це гарантує, що запити на читання не надсилаються в інші датацентри.
- `ANY`: Може відповісти будь-яка одна репліка, або координатор може зберегти підказку. Якщо підказка збережена, координатор пізніше спробує її повторно передати та доставити мутацію реплікам. Цей рівень консистентності приймається лише для операцій запису.
Операції запису завжди надсилаються на всі репліки, незалежно від рівня консистентності. Рівень консистентності лише контролює, скільки відповідей координатор чекатиме перед відповіддю клієнту.

Для операцій читання координатор зазвичай надсилає запити на читання лише на достатню кількість реплік для задоволення обраного рівня консистентності. Винятком є випадок із спекулятивним повторним запитом, коли додатковий запит може бути надісланий на резервну репліку, якщо початкові репліки не відповіли у встановлений час.

#### Вибір рівнів консистентності

Зазвичай рівні консистентності для читання та запису вибираються таким чином, щоб набори реплік перетиналися, забезпечуючи, що всі підтверджені записи будуть видимими для наступних читань. Це зазвичай виражається в термінах Dynamo як `W + R > RF`, де `W` — це рівень консистентності запису, `R` — рівень консистентності читання, а `RF` — коефіцієнт реплікації. Наприклад, якщо `RF = 3`, запит `QUORUM` вимагатиме відповідей від щонайменше `2 із 3` реплік. Якщо для запису та читання використовується `QUORUM`, хоча б одна репліка гарантовано братиме участь у обох запитах, що гарантує перетин кворумів, а отже, видимість запису для читання.

У багатодатацентровому середовищі `LOCAL_QUORUM` можна використовувати для забезпечення менш жорсткої, але корисної гарантії: запити на читання гарантовано побачать останній запис в межах одного датацентру, що часто є достатнім, оскільки клієнти, прив'язані до одного датацентру, зможуть читати власні записи.

Якщо жорстка консистентність не потрібна, можна використовувати нижчі рівні консистентності, такі як `LOCAL_ONE` або `ONE`, для підвищення пропускної здатності, зниження затримок і покращення доступності. У випадку реплікації між кількома датацентрами `LOCAL_ONE` зазвичай є менш доступним, ніж `ONE`, але працює швидше.

## Розподілене Членство Кластера та Виявлення Збоїв

Протоколи реплікації та розподілу даних у Cassandra залежать від знання, які вузли кластера активні або неактивні, щоб операції запису та читання могли бути оптимально спрямовані. Інформація про активність вузлів передається розподіленим чином за допомогою механізму виявлення збоїв, який базується на протоколі госсіп (gossip).

### Gossip

Протокол gossip у Cassandra використовується для поширення базової інформації про кластер, такої як склад учасників і версії міжвузлового мережевого протоколу. У системі gossip Cassandra вузли обмінюються інформацією не тільки про власний стан, а й про стан інших вузлів, які їм відомі. Ця інформація версіонується з використанням векторного годинника (vector clock), що складається з кортежів `(generation, version)`, де generation — це монотонний часовий штамп, а version — це логічний годинник, що приблизно збільшується щосекунди. Ці логічні годинники дозволяють Cassandra ігнорувати старі версії стану кластера, просто перевіряючи логічні значення у повідомленнях gossip.

Кожен вузол у Cassandra виконує gossip-завдання незалежно та періодично. Щосекунди кожен вузол у кластері:

- Оновлює свій стан (збільшуючи значення version) і створює своє локальне уявлення стану кластера.
- Обирає випадковий інший вузол у кластері для обміну станом gossip.
- Із певною ймовірністю намагається провести обмін з недоступними вузлами (якщо такі є).
- Виконує обмін даними з seed-вузлом, якщо цього не відбулося на кроці 2.

Коли оператор уперше ініціалізує Cassandra-кластер, він призначає деякі вузли як seed-вузли. Будь-який вузол може бути seed-вузлом, і єдина відмінність між seed- і не-seed-вузлами полягає в тому, що seed-вузли можуть додаватися до кільця (bootstrap) без необхідності підключення до інших seed-вузлів. Крім того, після запуску кластера seed-вузли стають гарячими точками для обміну gossip через крок 4.

Оскільки не-seed-вузли повинні мати змогу контактувати принаймні з одним seed-вузлом для того, щоб приєднатися до кластера, зазвичай включають кілька seed-вузлів, часто по одному для кожного стійка (rack) або датацентру. Seed-вузли часто обираються за допомогою наявних механізмів виявлення сервісів.

*Вузли кластера не обов’язково повинні мати узгоджений список seed-вузлів, і насправді після ініціалізації кластера нові вузли можуть бути налаштовані на використання будь-яких існуючих вузлів як seed. Основна перевага вибору одних і тих самих вузлів як seed полягає в тому, що це збільшує їх ефективність як гарячих точок для обміну даними через gossip.*

На даний момент протокол gossip також поширює метадані токенів і інформацію про версії схем. Ця інформація формує керуючий (control) рівень, що відповідає за планування передачі даних і синхронізацію схем. Наприклад, якщо вузол виявляє невідповідність у версії схеми через gossip, він ініціює завдання синхронізації схеми з іншими вузлами. Оскільки інформація про токени поширюється через gossip, вона також слугує контрольним механізмом для навчання вузлів про те, які кінцеві точки володіють певними даними.

### Кільцеве членство і виявлення збоїв

Gossip є основою кільцевого членства в Cassandra, але для визначення стану вузлів як активних (`UP`) чи неактивних (`DOWN`) використовується механізм детекції збоїв. Кожен вузол у Cassandra використовує варіант детектора збоїв [Phi Accrual Failure Detector](https://www.computer.org/csdl/proceedings-article/srds/2004/22390066/12OmNvT2phv), де кожен вузол незалежно приймає рішення про доступність інших вузлів. Це рішення базується головним чином на оновленнях heartbeat стану. Наприклад, якщо вузол не отримує нові heartbeat сигнали від певного вузла протягом заданого часу, детектор збоїв вважає цей вузол недоступним. Після цього Cassandra припиняє маршрутизацію запитів на читання до цього вузла (записи зазвичай записуються як "підказки" для подальшої доставки). Якщо/коли вузол знову починає надсилати heartbeat сигнали, Cassandra намагається відновити зв’язок і, якщо це вдається, позначає вузол як доступний.

*Стан активності вузлів (`UP` або `DOWN`) є локальним рішенням для кожного вузла і не поширюється через gossip. Heartbeat стан передається через gossip, але вузли не вважають один одного доступними, доки не зможуть успішно обмінятися повідомленнями через реальний мережевий канал.*

Cassandra ніколи не видаляє вузли з gossip стану без явного вказівки оператора, наприклад, під час операції виведення вузла з експлуатації (decommission) або при додаванні нового вузла з параметром `replace_address_first_boot`. Це дозволяє Cassandra тимчасово обробляти збої вузлів без потреби переналаштовувати розподіл даних, що запобігає одночасним переміщенням діапазонів (range movements) у кільці, які можуть призвести до порушення монотонної консистентності або навіть до втрати даних.

## Масштабування за рахунок додавання вузлів на обладнанні загального призначення

Cassandra підтримує масштабування для задоволення зростаючих потреб у обсягах даних та частоті запитів. Масштабування за рахунок додавання вузлів означає, що до кільця додаються нові вузли, причому кожен новий вузол лінійно збільшує обчислювальну потужність і обсяг зберігання. На відміну від цього, вертикальне масштабування (scale-up) передбачає збільшення ресурсів на існуючих вузлах бази даних. Cassandra також підтримує масштабування вгору, і в деяких випадках воно може бути кращим залежно від середовища розгортання. Таким чином, Cassandra надає операторам гнучкість вибору між горизонтальним і вертикальним масштабуванням.

Однією з ключових характеристик Dynamo, якої дотримується Cassandra, є робота на обладнанні загального призначення, тому багато інженерних рішень приймаються саме з урахуванням цього. Наприклад, Cassandra передбачає, що вузли можуть виходити з ладу в будь-який час, автоматично налаштовується для оптимального використання доступних ресурсів процесора та пам'яті та активно використовує передові методи стиснення та кешування, щоб максимально ефективно використовувати пам'ять і дисковий простір.

### Проста модель запитів

Як і Dynamo, Cassandra не підтримує транзакції, які охоплюють кілька розділів даних, що є звичайним явищем у системах керування реляційними базами даних SQL. Це забезпечує простіший API для операцій читання та запису, дозволяючи Cassandra легше масштабуватися горизонтально, оскільки транзакції з кількома розділами, які охоплюють кілька вузлів, зазвичай важко реалізувати й вони створюють затримки.

Натомість Cassandra надає швидкі та узгоджені операції з низькою затримкою для запитів по одному розділу, що дозволяє отримувати як весь розділ, так і його підмножини на основі фільтрів за первинним ключем. Крім того, Cassandra підтримує функціональність операцій порівняння та заміни для одного розділу через CQL API легких транзакцій.

### Простий інтерфейс для зберігання записів

Cassandra пропонує інтерфейс зберігання даних, який є складнішим, ніж прості сховища "ключ-значення", але значно простішим за SQL-реляційні моделі. Це сховище даних типу wide-column store, де розділи даних містять кілька рядків, кожен з яких складається з набору колонок з гнучким типом. Кожен рядок ідентифікується унікальним ключем розділу та одним або кількома кластерними ключами, і кожен рядок може мати стільки колонок, скільки потрібно.

Це дозволяє користувачам гнучко додавати нові колонки до існуючих наборів даних у міру виникнення нових вимог. Зміни в схемі вносяться як зміни метаданих і можуть виконуватися паралельно з живим навантаженням, тому користувачі можуть безпечно додавати колонки до баз даних Cassandra, не боячись погіршення продуктивності запитів.